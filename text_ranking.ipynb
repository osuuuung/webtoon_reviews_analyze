{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006bae07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install PyKomoran\n",
    "import pandas as pd\n",
    "import re\n",
    "from PyKomoran import *\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ee1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 파일 불러오기\n",
    "comment_df = pd.read_excel('여름의 너에게.xlsx')\n",
    "\n",
    "# 불용어 리스트 만들기\n",
    "f=open('stopwords.txt','r',encoding='utf-8')\n",
    "stop_words = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1776c59a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Komoran tokenizer\n",
    "komoran = Komoran(\"STABLE\")\n",
    "\n",
    "# Preprocessing function using KoNLPy2\n",
    "def preprocess_text(text, stop_words):\n",
    "    # Remove non-Korean characters and whitespace\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "    # Tokenize the text using Komoran\n",
    "    morphs_and_tags = komoran.get_plain_text(text).split()\n",
    "    morphs = [mt.split('/')[0] for mt in morphs_and_tags]\n",
    "    # Remove stopwords\n",
    "    words = [word for word in morphs if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "word_list = []\n",
    "for text in comment_df['review']:\n",
    "    words = preprocess_text(text,stop_words)\n",
    "    word_list.append(words)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca3d899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                             [시골, 너무, 좋]\n",
      "1                                         [아따, 말, 잘생기, 엇]\n",
      "2               [잘생기, ㄴ, 사람, 은, 그림, 체, 뽀, 요, 로맨스, 시작, ㄴ가]\n",
      "3                                                [공포물, 았]\n",
      "4                               [그림, 체, 말, 니, 바, 끼, 엇, ㅇ]\n",
      "                              ...                        \n",
      "4151    [작가, 님, 일러스트, 사랑, 합, 미, 다, 열심히, 정, 주행, 고, 기다리,...\n",
      "4152                       [머리, 기른, 태민이, 미쳤다, 넘, 잘생겼어요, ]\n",
      "4153                      [고등학생, 되, ㄴ, 애기, 시즌, 기대, 되, ㄴ다]\n",
      "4154                                        [처음, 다, 보, 았]\n",
      "4155                                            [댓, 글, 빠]\n",
      "Name: preprocessed, Length: 4156, dtype: object\n"
     ]
    }
   ],
   "source": [
    "comment_df['preprocessed'] = word_list\n",
    "print(comment_df['preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83789b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 그래프를 편집하는 함수\n",
    "def word_graph(sentence):\n",
    "    # Create a graph representation of the sentences\n",
    "    \n",
    "    for word in sentence:\n",
    "        graph.add_node(word)\n",
    "    for i in range(len(sentence)):\n",
    "        for j in range(i+1, len(sentence)):\n",
    "            word1, word2 = sentence[i], sentence[j]\n",
    "            if graph.has_edge(word1, word2):\n",
    "                graph[word1][word2]['weight'] += 1\n",
    "            else:\n",
    "                graph.add_edge(word1, word2, weight=1)\n",
    "\n",
    "                \n",
    "# 텍스트 랭킹 함수\n",
    "def text_ranking(graph,num_keywords=50, damping_factor=0.85, max_iter=100, tol=1e-4):\n",
    "\n",
    "    # PageRank algorithm to get keywords\n",
    "    pagerank_scores = nx.pagerank(graph, alpha=damping_factor, max_iter=max_iter, tol=tol)\n",
    "\n",
    "    # Sort the keywords based on their PageRank scores\n",
    "    keywords = sorted(pagerank_scores, key=pagerank_scores.get, reverse=True)[:num_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.Graph()\n",
    "\n",
    "for sentence in comment_df['preprocessed']:\n",
    "    word_graph(sentence)\n",
    "\n",
    "keywords = text_ranking(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7d16086",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '는', 'ㄴ', '다', '태민', '고', '너무', 'ㄹ', '았', '윤이', '소라', '되', '안', '주', '지', '도', '윤', '었', '있', '윤아', '은', '보', '게', '태', '좋', '귀엽', '진짜', '민이', '잘', '같', '남', '는데', 'ㄴ다', '그냥', '님', '작가', '면', '겠', 'ㅁ', '제발', '시', '말', '랑', '계속', '더', '내', '뺏기', '좋아하', '기', 'ㄴ데']\n"
     ]
    }
   ],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65619e4",
   "metadata": {},
   "source": [
    "### 웹툰 특성 상 가장 많은 내용은 누구 예쁘다 잘생겼다, 제발 더 내주세요 등 분석에 필요없는 내용이 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b04e10",
   "metadata": {},
   "source": [
    "### 추출한 키워드를 토대로 품사 단위로 구분, 이를 이용해서 문장 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_keywords(keywords):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55839cfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'keywords'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'keywords'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4908\\2626161156.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Get overall keyword analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0moverall_keywords_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyze_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keywords'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Example: Top 10 most frequent keywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'keywords'"
     ]
    }
   ],
   "source": [
    "# Analysis on the keywords\n",
    "def analyze_keywords(keywords_list):\n",
    "    all_keywords = [keyword for keywords in keywords_list for keyword in keywords]\n",
    "    keyword_freq = defaultdict(int)\n",
    "    for keyword in all_keywords:\n",
    "        keyword_freq[keyword] += 1\n",
    "    sorted_freq = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_freq\n",
    "\n",
    "# Get overall keyword analysis\n",
    "overall_keywords_freq = analyze_keywords(keywords)\n",
    "\n",
    "# Example: Top 10 most frequent keywords\n",
    "top_10_keywords = overall_keywords_freq[:10]\n",
    "print(\"Top 10 most frequent keywords:\")\n",
    "for keyword, frequency in top_10_keywords:\n",
    "    print(f\"{keyword}: {frequency} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb3019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
